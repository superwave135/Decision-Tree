{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Name: Tan Ngiap Chuan Alvin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Machine Learning - Practicum # - Decision Tree (ID3)\n",
    "\n",
    "**Topics covered**: Build a Decision Tree from scratch\n",
    "\n",
    "**Deliverables**:\n",
    "- Complete the tasks as detailed in this document.\n",
    "- You are not allowed to use any Machine Learning APIs for this practicum (NumPy and Pandas are allowed).\n",
    "\n",
    "**Objectives**:  \n",
    "Decision trees are a powerful prediction method and extremely popular. Although the idea behind it is comparatively straightforward, implementing the algorithm from scratch is not as easy. This tutorial will help you code an ID3 algorithm on a toy dataset step by step. After completing it, you will know:\n",
    "- How to calculate the entropy and information gain.\n",
    "- How to apply the information gain to determine the root node for a sub-tree.\n",
    "- How to build a decision tree classifier by using ID3(Iterative Dichotomiser 3) algorithm.\n",
    "- How to apply the tree/classifier to a given problem.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Dataset\n",
    "We'll again use the data set for practicing Naive Bayes Classifier. <br>\n",
    "Run the following cell to load the dataset and import the relevant libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Weather      Car   Class\n",
      "0   sunny  working  go-out\n",
      "1   rainy   broken  go-out\n",
      "2   sunny  working  go-out\n",
      "3   sunny  working  go-out\n",
      "4   sunny  working  go-out\n",
      "(10, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "print(df.head())\n",
    "print(df.shape) # own"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset describes two categorical input variables and a class variable that has two outputs:\n",
    "\n",
    "| Weather | Car     | Class     |\n",
    "|---------|---------|-----------|\n",
    "| sunny   | working | go-out    |\n",
    "| rainy   | broken  | go-out    |\n",
    "| sunny   | working | go-out    |\n",
    "| sunny   | working | go-out    |\n",
    "| sunny   | working | go-out    |\n",
    "| rainy   | broken  | stay-home |\n",
    "| rainy   | broken  | stay-home |\n",
    "| sunny   | working | stay-home |\n",
    "| sunny   | broken  | stay-home |\n",
    "| rainy   | broken  | stay-home |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoder(df):\n",
    "    '''\n",
    "    function takes in the dataset `df` as input and returns the encoded dataset `edf` as output.\n",
    "    '''\n",
    "    # converting to binary data \n",
    "    df_weather = pd.get_dummies(df[\"Weather\"]) \n",
    "    df_weather = df_weather.drop(['rainy'], axis=1)\n",
    "    df_weather = df_weather.rename(columns={\"sunny\": \"Weather\"})\n",
    "#     print(df_weather)\n",
    "    \n",
    "    df_car = pd.get_dummies(df[\"Car\"]) \n",
    "    df_car = df_car.drop(['broken'], axis=1)\n",
    "    df_car = df_car.rename(columns={\"working\": \"Car\"})\n",
    "#     print(df_car)\n",
    "    \n",
    "    df_class = pd.get_dummies(df[\"Class\"]) \n",
    "    df_class = df_class.drop(['stay-home'], axis=1)\n",
    "    df_class = df_class.rename(columns={\"go-out\": \"Class\"})\n",
    "#     print(df_class)\n",
    "    \n",
    "#     # display result \n",
    "    df_encoded = pd.concat((df_weather, df_car, df_class), axis=1) \n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Weather  Car  Class\n",
      "0        1    1      1\n",
      "1        0    0      1\n",
      "2        1    1      1\n",
      "3        1    1      1\n",
      "4        1    1      1\n",
      "5        0    0      0\n",
      "6        0    0      0\n",
      "7        1    1      0\n",
      "8        1    0      0\n",
      "9        0    0      0 <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "df = label_encoder(df)\n",
    "print(df, type(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Implement ID3\n",
    "We're going to code an ID3 algorithm on the above data set. The algorithm uses the information gain to find the feature that maximises it and make a split based on that feature, while the information gain is based on entropy. We first implement the entropy computation. \n",
    "\n",
    "### 2.1 Entropy\n",
    "We can compute the entropy with the following formula:<br>\n",
    "\n",
    "$$\n",
    "H(S) = -P(\\text{go-out})log_2P(\\text{go-out})-P(\\text{stay-home})log_2P(\\text{stay-home}) \\\\\n",
    "$$\n",
    "In order to return the entropy for the gvien data set and any of its subset. The function `get_entropy` should take in the entire dataset, a list of indices for specifying the subset as input and returns the entropy of the set/subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy(data, idx):\n",
    "    \n",
    "    # parameter: dataFrame data\n",
    "    # parameter: list      idx\n",
    "    # return:    float     entropy    \n",
    "    # Your code goes here\n",
    "    alist= [i for i in idx]       \n",
    "    counts_class = np.bincount(data.iloc[alist, 2]) # Equivalent to data[rows, 'Class']\n",
    "    probabilities = counts_class / len(alist)       # Divide by the total column length to get a probability\n",
    "    entropy = 0                                     # Initialize the entropy to 0\n",
    "    for prob in probabilities:                      # Loop through the probabilities, and add each one to the total entropy\n",
    "        if prob > 0:\n",
    "            entropy -= prob * math.log(prob,2)      # Use log from math and set base to 2\n",
    "        \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to validate the function, expected output is 1.0, 0.0 and 0.9182.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0 0.9182958340544896\n"
     ]
    }
   ],
   "source": [
    "e1 = get_entropy(df, range(0,10))\n",
    "e2 = get_entropy(df, range(5,10))\n",
    "e3 = get_entropy(df,[0,2,3,4,7,8])\n",
    "print(e1, e2, e3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Information Gain\n",
    "Now we are to compute the information gain to choose the feature that maximises it and then make the split based on that feature. The information gain is defined as the total entropy minus the entropy if we chose a particular feature *j*.\n",
    "\n",
    "$$\n",
    "G(S,j)=H(S)-\\sum_j\\frac{|S_j|}{|S|}H(S_j)\n",
    "$$\n",
    "\n",
    "Write a function `get_information_gain` that takes in entire dataset, a list of indices for specifying the current set, and the *j*-th feature id. It returns the information gain by choosing the j-th feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_information_gain(data, idx, feature_id):\n",
    "    \n",
    "    # parameter: dataFrame data\n",
    "    # parameter: list      idx\n",
    "    # parameter: int       feature_id\n",
    "    # return:    float     information_gain for the feature_id\n",
    "    \n",
    "    # Your code goes here\n",
    "    \"\"\"\n",
    "    Calculate information gain given a data set, column to split on, and target\n",
    "    \"\"\"\n",
    "    original_entropy = get_entropy(data, idx) # Calculate the original entropy\n",
    "    \n",
    "    values = data.iloc[:, feature_id].unique() # Find the unique values in the column   \n",
    "    \n",
    "    left_split = df[df.iloc[:, feature_id] == values[0]]  # Make two subsets of the data, based on the unique values\n",
    "    right_split = df[df.iloc[:, feature_id] == values[1]] # Make two subsets of the data, based on the unique values\n",
    "    \n",
    "    to_subtract = 0\n",
    "    for subset in [left_split, right_split]: # Loop through the splits and calculate the subset entropies\n",
    "        prob = (subset.shape[0] / data.shape[0]) \n",
    "        to_subtract += prob * get_entropy(data, subset.index)\n",
    "    \n",
    "    return original_entropy - to_subtract # Return information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, run the following cell, you should get: 0.1245.. and 0.2780.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12451124978365313\n",
      "0.2780719051126377\n"
     ]
    }
   ],
   "source": [
    "print(get_information_gain(df, range(10),0))\n",
    "print(get_information_gain(df, range(10),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Get best-feature\n",
    "ID3 algorithm chooses the feature that maximise the information gain at each split. `get_information_gain()` only calculates the information gain for a given set and a particular feature. We need to create another function to find the best feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds the feature that maximizes the information gain.\n",
    "def get_best_feature(data, idx, features_id_list):\n",
    "    \n",
    "    # parameter:  dataFrame       data\n",
    "    # parameter:  list            idx\n",
    "    # parameter:  list            list of features id\n",
    "    # return:     int             feature_id of the feature that maximizes the information gain\n",
    "    \n",
    "    # Your code goes here\n",
    "    information_gains = {}                                      # Intialize an empty dictionary for information gains\n",
    "  \n",
    "    for col in features_id_list:                                # Iterate through each column name in our list\n",
    "        information_gain = get_information_gain(data, idx, col) # Find the information gain for the column                             \n",
    "        information_gains[col] = information_gain               # Add the information gain to our dictionary using the column name as the key\n",
    "                                         \n",
    "    return max(information_gains, key=information_gains.get)    # Return the key with the highest value   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected output of the following function is: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(get_best_feature(df, range(10),[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Build up the Tree\n",
    "<img src=\"tree.png\" width=\"300\" height='151'>\n",
    "So far we got all functions to build up a decision tree. Now the data structure of a tree need to be defined <br>\n",
    "Above figure shows an instance of Tree. The member values of the root node is: <br>\n",
    "- name = 'car'   <br>\n",
    "- leaf = 0       <br>\n",
    "- children = [TreeNode(\"weather\"), TreeNode(\"stay_home\")]  <br>\n",
    "- edges = ['working', 'broken]  <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class treeNode:\n",
    "    def __init__(self):\n",
    "        self.name = None              # either feature name or class label\n",
    "        self.leaf = 0                 # = 1 when the value is a class label\n",
    "        self.children = []            # list of children nodes when leaf is 0 and name is a feature name\n",
    "        self.edges = []               # edge connecting current node and its children\n",
    "                                      # size of edges is the same as the size of children\n",
    "                                      # elements in edges are all possible values of the chosen feature "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're creating a function to initiate the process. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_id3(data):\n",
    "\n",
    "    # Get the initial index, i.e., list of indices for all samples\n",
    "    idx = range(len(data))\n",
    "    print(f'idx: {idx}')  # eg. idx: range(0, 10)\n",
    "    # Get the initial features id\n",
    "    features_id = [x for x in range(len(data.columns)-1)] # a list of features id\n",
    "    print(f'features_id: {features_id}') # eg. features_id: [0, 1]\n",
    "    # Build up the tree by recursively selecting features\n",
    "    tree = id3_recursive(data, idx, features_id, treeNode()) \n",
    "    print(f'tree:{tree}')\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, completing the recursive function `id3` to finishi the implementation of tree building. See the [pseudocode](https://en.wikipedia.org/wiki/ID3_algorithm) from wikipedia for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id3_recursive(data, idx, features_id, node): # instantiate node as treeNode()\n",
    "    \n",
    "    # Create a root node for the tree\n",
    "    # node is the root node passed in \n",
    "\n",
    "    # Obtain the list of class_labels\n",
    "    class_labels = data[data.columns[-1]].to_list()\n",
    "    class_labels = [class_labels[i] for i in idx]\n",
    "    class_labels_unique = list(set(class_labels))\n",
    "\n",
    "    # Two Cases for touching the bottom and return\n",
    "    \n",
    "    # CASE 1 Pure crystal: if all samples are in the same class\n",
    "    # Label the node value as the class label and return\n",
    "    if len(class_labels_unique) == 1:\n",
    "        node.name = class_labels_unique[0]\n",
    "        node.leaf = 1\n",
    "        return node\n",
    "    \n",
    "    # CASE 2 All features are used: no more features waiting for choosing\n",
    "    # Lable the node value as the most likely class and return\n",
    "    # Your code goes here\n",
    "    if len(features_id) == 0:\n",
    "        subset = data.iloc[idx]\n",
    "        # group by count max; assign as final class \n",
    "        node.name = subset.groupby(subset.columns[-1]).count().idxmax().values[0]\n",
    "        node.leaf = 1\n",
    "        return node\n",
    "    \n",
    "    \n",
    "    # Otherwise,  \n",
    "    # Choose the feature that maximizes the information gain, and\n",
    "    # Loop through all the values to build up sub-tree\n",
    "    print(f'------------------------')\n",
    "    # Your code goes here  \n",
    "    best_feature = get_best_feature(data, idx, features_id)\n",
    "    print(f'\\nbest_feature: {best_feature}') # best_feature: 1\n",
    "    \n",
    "    # feature name\n",
    "    node.name = data.columns[best_feature]\n",
    "    print(f'node.name: {node.name}') # node.name: Car\n",
    "    features_id.remove(best_feature)\n",
    "    \n",
    "    # create children\n",
    "    subset = data.iloc[idx]\n",
    "    print(f'subset: \\n{subset}') # dataframe \n",
    "    \n",
    "    # named feature, get from entire dataset, not subset, since there are cases where it goes empty\n",
    "    # if getting only the values in subset: best_features_values = subset[subset.columns[best_feature]].unique().tolist()\n",
    "    best_features_values = data[data.columns[best_feature]].unique().tolist()\n",
    "    print(f'best_features_values: {best_features_values}') # eg. best_features_values: [1, 0]\n",
    "    \n",
    "    # depth first?\n",
    "    for feature in best_features_values:\n",
    "        print(f'feature: {feature}')\n",
    "        feature_idx = subset.loc[subset[subset.columns[best_feature]] == feature].index\n",
    "        print(f'feature_idx: {feature_idx}') # eg. feature_idx: Int64Index([0, 2, 3, 4, 7], dtype='int64')\n",
    "        node.edges.append(feature)\n",
    "        print(f'append the feature: {feature}') # eg. append the feature: 1\n",
    "        \n",
    "        # if empty, add leaf node labelled with most common value in the dataset\n",
    "        if not len(feature_idx):\n",
    "            leafNode = treeNode()\n",
    "            # most common class in subset\n",
    "            leafNode.name = subset.groupby(subset.columns[-1]).count().idxmax().values[0]\n",
    "            leafNode.leaf = 1\n",
    "            node.children.append(leafNode)\n",
    "            print(f'append the leafNode: {leafNode}')\n",
    "            \n",
    "        else:\n",
    "            node.children.append(id3_recursive(data, feature_idx, features_id, treeNode()))\n",
    "    # node.children node.edges\n",
    "    return node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the decision tree, define your function `print_decision_tree` in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function print_decision_tree, your code goes here\n",
    "def print_decision_tree():\n",
    "    print('                   Car')\n",
    "    print('           working/   \\\\')\n",
    "    print('                 /     \\\\')\n",
    "    print(\"            Weather     stay-home\")\n",
    "    print('               /  \\\\')\n",
    "    print(\"         sunny/    \\\\rainy\")\n",
    "    print('             /      \\\\')\n",
    "    print('       go-out         go-out')\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: range(0, 10)\n",
      "features_id: [0, 1]\n",
      "------------------------\n",
      "\n",
      "best_feature: 1\n",
      "node.name: Car\n",
      "subset: \n",
      "   Weather  Car  Class\n",
      "0        1    1      1\n",
      "1        0    0      1\n",
      "2        1    1      1\n",
      "3        1    1      1\n",
      "4        1    1      1\n",
      "5        0    0      0\n",
      "6        0    0      0\n",
      "7        1    1      0\n",
      "8        1    0      0\n",
      "9        0    0      0\n",
      "best_features_values: [1, 0]\n",
      "feature: 1\n",
      "feature_idx: Int64Index([0, 2, 3, 4, 7], dtype='int64')\n",
      "append the feature: 1\n",
      "------------------------\n",
      "\n",
      "best_feature: 0\n",
      "node.name: Weather\n",
      "subset: \n",
      "   Weather  Car  Class\n",
      "0        1    1      1\n",
      "2        1    1      1\n",
      "3        1    1      1\n",
      "4        1    1      1\n",
      "7        1    1      0\n",
      "best_features_values: [1, 0]\n",
      "feature: 1\n",
      "feature_idx: Int64Index([0, 2, 3, 4, 7], dtype='int64')\n",
      "append the feature: 1\n",
      "feature: 0\n",
      "feature_idx: Int64Index([], dtype='int64')\n",
      "append the feature: 0\n",
      "append the leafNode: <__main__.treeNode object at 0x00000241DC7B0100>\n",
      "feature: 0\n",
      "feature_idx: Int64Index([1, 5, 6, 8, 9], dtype='int64')\n",
      "append the feature: 0\n",
      "tree:<__main__.treeNode object at 0x00000241DC7B0370>\n",
      "                   Car\n",
      "           working/   \\\n",
      "                 /     \\\n",
      "            Weather     stay-home\n",
      "               /  \\\n",
      "         sunny/    \\rainy\n",
      "             /      \\\n",
      "       go-out         go-out\n"
     ]
    }
   ],
   "source": [
    "# build the decision tree from data set\n",
    "tree = decision_tree_id3(df)\n",
    "\n",
    "# Print the tree\n",
    "# Your code goes here\n",
    "print_decision_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Make Prediction with the Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tree, xquery): # where tree is decision_tree_id3(df), xquery is df\n",
    "    \n",
    "    # your code goes here\n",
    "    predictions = []\n",
    "    print(f'xquery:\\n{xquery}')\n",
    "    for _, row in xquery.iterrows():\n",
    "        # traverse tree using cols of xquery\n",
    "        node = tree\n",
    "        while node.leaf == 0:\n",
    "            # get edge matching, return idx\n",
    "            childIdx = node.edges.index(row[node.name]) # self.name = None, either feature name or class label\n",
    "            # traverse that child\n",
    "            node = node.children[childIdx] # where self.children = [] is list of children nodes when leaf is 0\n",
    "        # arrived at leaf\n",
    "        predictions.append(node.name) # self.name = None, either feature name or class label\n",
    "        print(f'predictions:\\n{predictions}')\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xquery:\n",
      "   Weather  Car  Class\n",
      "0        1    1      1\n",
      "1        0    0      1\n",
      "2        1    1      1\n",
      "3        1    1      1\n",
      "4        1    1      1\n",
      "5        0    0      0\n",
      "6        0    0      0\n",
      "7        1    1      0\n",
      "8        1    0      0\n",
      "9        0    0      0\n",
      "predictions:\n",
      "[1]\n",
      "predictions:\n",
      "[1, 0]\n",
      "predictions:\n",
      "[1, 0, 1]\n",
      "predictions:\n",
      "[1, 0, 1, 1]\n",
      "predictions:\n",
      "[1, 0, 1, 1, 1]\n",
      "predictions:\n",
      "[1, 0, 1, 1, 1, 0]\n",
      "predictions:\n",
      "[1, 0, 1, 1, 1, 0, 0]\n",
      "predictions:\n",
      "[1, 0, 1, 1, 1, 0, 0, 1]\n",
      "predictions:\n",
      "[1, 0, 1, 1, 1, 0, 0, 1, 0]\n",
      "predictions:\n",
      "[1, 0, 1, 1, 1, 0, 0, 1, 0, 0]\n",
      "[1, 0, 1, 1, 1, 0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "prediction = predict(tree, df)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "print('accuracy:', (df['Class'] == prediction).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "5    0\n",
      "6    0\n",
      "7    0\n",
      "8    0\n",
      "9    0\n",
      "Name: Class, dtype: uint8\n"
     ]
    }
   ],
   "source": [
    "print(df['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
